import os, json, requests
from loguru import logger
from openai import OpenAI

# --- Environment setup ---
USE_LOCAL_OLLAMA = os.getenv("USE_LOCAL_OLLAMA", "false").lower() == "true"
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
PORTKEY_API_KEY = os.getenv("PORTKEY_API_KEY")
PORTKEY_BASE_URL = os.getenv("PORTKEY_BASE_URL", "https://api.portkey.ai")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def plan_agent(prompt: str) -> dict:
    """
    Generate a ChipLoop-compliant agent file from high-level user intent.
    Uses LLM fallback hierarchy identical to all domain agents:
    Ollama ‚Üí Portkey ‚Üí OpenAI ‚Üí Static fallback.
    """

    logger.info(f"üß¨ Planning new agent for goal: {prompt[:120]}")

    # --- ChipLoop template embedded directly in prompt ---
    agent_template = """
You are an AI Agent Architect for ChipLoop.

Every agent must follow this structure with identical imports, logging, and fallback logic.

---
TEMPLATE START
import os, json, requests
from loguru import logger
from openai import OpenAI

USE_LOCAL_OLLAMA = os.getenv("USE_LOCAL_OLLAMA", "false").lower() == "true"
PORTKEY_API_KEY = os.getenv("PORTKEY_API_KEY")
PORTKEY_BASE_URL = os.getenv("PORTKEY_BASE_URL", "https://api.portkey.ai")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

async def run_agent(work_dir: str, input_files: dict, prompt: str = ""):
    \"\"\"ChipLoop AI Agent ‚Äî auto-generated by the AI Agent Planner.\"\"\"
    log_path = os.path.join(work_dir, "{agent_name}.log")
    artifact_path = os.path.join(work_dir, "{output_file}")
    os.makedirs(work_dir, exist_ok=True)
    logger.add(log_path)
    logger.info("üöÄ Running {agent_name} ...")

    try:
        # --- Custom Logic Starts ---
        # Replace this section with the agent's core logic.
        pass
        # --- Custom Logic Ends ---
    except Exception as e:
        logger.error(f"‚ùå {agent_name} failed: {e}")
        raise e

    logger.success("‚úÖ {agent_name} completed successfully.")
    return {"log": log_path, "artifact": artifact_path}
TEMPLATE END
---

Respond with valid JSON only:
{
  "agent_name": "...",
  "domain": "...",
  "inputs": ["..."],
  "outputs": ["..."],
  "description": "...",
  "full_code": "<COMPLETE PYTHON CODE>"
}
"""

    combined_prompt = f"{agent_template}\nUser goal: {prompt}"

    # --- Step 1: Try Ollama (Local) ---
    spec_json = None
    if USE_LOCAL_OLLAMA:
        try:
            logger.info("ü¶ô Using local Ollama for agent generation...")
            payload = {"model": "llama3", "prompt": combined_prompt}
            response = requests.post(OLLAMA_URL, json=payload, timeout=600)
            spec_json = response.text
            return safe_parse_agent(spec_json)
        except Exception as e:
            logger.warning(f"Ollama failed: {e}")

    # --- Step 2: Try Portkey ---
    if PORTKEY_API_KEY:
        try:
            logger.info("ü™Ñ Using Portkey for agent generation...")
            headers = {
                "x-portkey-api-key": PORTKEY_API_KEY,
                "Content-Type": "application/json",
            }
            r = requests.post(
                f"{PORTKEY_BASE_URL}/v1/chat/completions",
                json={
                    "model": "gpt-4o-mini",
                    "messages": [{"role": "user", "content": combined_prompt}],
                    "temperature": 0.3,
                },
                headers=headers,
                timeout=90,
            )
            content = r.json()["choices"][0]["message"]["content"]
            return safe_parse_agent(content)
        except Exception as e:
            logger.warning(f"Portkey failed: {e}")

    # --- Step 3: Try OpenAI ---
    if OPENAI_API_KEY:
        try:
            logger.info("üåê Using OpenAI fallback for agent generation...")
            client = OpenAI(api_key=OPENAI_API_KEY)
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": combined_prompt}],
                temperature=0.3,
            )
            content = resp.choices[0].message.content
            return safe_parse_agent(content)
        except Exception as e:
            logger.error(f"OpenAI failed: {e}")

    # --- Step 4: Static fallback ---
    logger.error("‚ùå All LLM backends failed ‚Äî returning fallback agent.")
    fallback = {
        "agent_name": "FallbackAgent",
        "domain": "generic",
        "inputs": [],
        "outputs": ["output.txt"],
        "description": "Static fallback agent used due to LLM failure.",
        "full_code": "# Fallback agent placeholder\nasync def run_agent(work_dir, input_files, prompt=''): pass",
    }
    return fallback


def safe_parse_agent(text: str) -> dict:
    """Extract and parse JSON safely from model response."""
    try:
        start, end = text.find("{"), text.rfind("}") + 1
        j = text[start:end]
        data = json.loads(j)
        if not isinstance(data, dict) or "agent_name" not in data:
            raise ValueError("Invalid JSON structure")
        logger.success(f"‚úÖ Generated agent: {data.get('agent_name')}")
        return data
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è JSON parse failed: {e}")
        return {
            "agent_name": "UnknownAgent",
            "domain": "unknown",
            "inputs": [],
            "outputs": [],
            "description": "Invalid JSON",
            "full_code": text[:1000],
        }
# --- Simple async wrapper for plan_agent ---
async def plan_agent_fallback(prompt: str) -> dict:
    """
    Async-compatible fallback to generate a new agent using the existing plan_agent().
    """
    try:
        agent = plan_agent(prompt)
        return agent
    except Exception as e:
        logger.error(f"‚ùå plan_agent_fallback failed for {prompt}: {e}")
        return {
            "agent_name": "UnknownAgent",
            "domain": "unknown",
            "inputs": [],
            "outputs": [],
            "description": "Agent generation failed.",
            "full_code": "",
        }
