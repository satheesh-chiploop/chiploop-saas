import os
import json
import glob
import shutil
import subprocess
from datetime import datetime

from utils.artifact_utils import save_text_artifact_and_record

AGENT_NAME = "Digital Synthesis Agent"

DEFAULT_PDK_VARIANT = os.getenv("CHIPLOOP_PDK_VARIANT", "sky130A")
DEFAULT_OPENLANE_IMAGE = os.getenv("CHIPLOOP_OPENLANE_IMAGE", "ghcr.io/efabless/openlane2:2.4.0.dev1")
DEFAULT_NUM_CORES = int(os.getenv("OPENLANE_NUM_CORES", "2"))

def _ensure_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)

def _read_json(path: str) -> dict:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _pick_clock(spec: dict) -> tuple[str, float]:
    """
    Returns (clock_name, clock_period_ns)
    Best-effort parsing, never fake precision.
    """
    # Try common shapes
    clk_name = "clk"
    clk_period = 10.0  # 100MHz default

    # examples you might have later:
    # spec["clock"]["name"], spec["clock"]["period_ns"], spec["clock"]["freq_mhz"]
    c = spec.get("clock") or {}
    if isinstance(c, dict):
        if c.get("name"):
            clk_name = str(c["name"])
        if c.get("period_ns"):
            try:
                clk_period = float(c["period_ns"])
            except Exception:
                pass
        elif c.get("freq_mhz"):
            try:
                mhz = float(c["freq_mhz"])
                if mhz > 0:
                    clk_period = 1000.0 / mhz
            except Exception:
                pass

    return clk_name, clk_period

def _write_local(path: str, content: str) -> None:
    _ensure_dir(os.path.dirname(path))
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)

def _run(cmd: list[str], cwd: str | None = None) -> tuple[int, str]:
    p = subprocess.run(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    return p.returncode, p.stdout

def run_agent(state: dict) -> dict:
    print(f"\nüèÅ Running {AGENT_NAME}...")

    workflow_id = state.get("workflow_id", "default")
    workflow_dir = state.get("workflow_dir") or f"backend/workflows/{workflow_id}"
    _ensure_dir(workflow_dir)

    # ---------- Locate RTL ----------
    rtl_files = []
    artifact_list = state.get("artifact_list") or []
    if isinstance(artifact_list, list) and artifact_list:
        rtl_files = [p for p in artifact_list if p and os.path.exists(p)]
    else:
        single = state.get("artifact")
        if single and os.path.exists(single):
            rtl_files = [single]

    if not rtl_files:
        # fallback: any .v under workflow_dir
        rtl_files = sorted(glob.glob(os.path.join(workflow_dir, "*.v")))

    if not rtl_files:
        raise FileNotFoundError(f"No RTL found for synthesis in {workflow_dir}")

    # ---------- Spec JSON (optional) ----------
    spec = {}
    spec_json = state.get("spec_json")
    if spec_json and os.path.exists(spec_json):
        spec = _read_json(spec_json)

    clk_name, clk_period_ns = _pick_clock(spec)

    # ---------- Stage folder ----------
    stage_dir = os.path.join(workflow_dir, "digital", "synth")
    rtl_dir = os.path.join(stage_dir, "rtl")
    constraints_dir = os.path.join(stage_dir, "constraints")
    logs_dir = os.path.join(stage_dir, "logs")
    _ensure_dir(rtl_dir)
    _ensure_dir(constraints_dir)
    _ensure_dir(logs_dir)

    # Copy RTL into deterministic local folder (avoid container path issues)
    copied = []
    for f in rtl_files:
        dst = os.path.join(rtl_dir, os.path.basename(f))
        if os.path.abspath(f) != os.path.abspath(dst):
            shutil.copy2(f, dst)
        copied.append(dst)

    # Pick top module name best-effort:
    # - If your spec later contains block.name or top_module, use it.
    top_module = (
        (spec.get("top_module") if isinstance(spec, dict) else None)
        or ((spec.get("block") or {}).get("name") if isinstance(spec.get("block"), dict) else None)
        or state.get("top_module")
        or "top"
    )
    top_module = str(top_module).strip() or "top"

    # ---------- SDC ----------
    sdc_path = os.path.join(constraints_dir, "top.sdc")
    sdc = f"""# Auto-generated by {AGENT_NAME}
# Workflow: {workflow_id}
# NOTE: If you know the real clock ports/constraints, override in UI later.

create_clock -name {clk_name} -period {clk_period_ns:.3f} [get_ports {{{clk_name}}}]
set_clock_uncertainty 0.2 [get_clocks {clk_name}]
"""
    _write_local(sdc_path, sdc)

    # ---------- OpenLane2 config.json ----------
    # Keep it minimal and explicit.
    # PDK selection is via CLI/env; do NOT hardcode absolute host paths here.
    config_path = os.path.join(stage_dir, "config.json")

    # OpenLane2 supports JSON configs; we keep sources relative inside the mounted /work
    verilog_sources = [f"rtl/{os.path.basename(p)}" for p in copied]

    config = {
        "meta": {
            "name": top_module,
            "generated_by": AGENT_NAME,
            "generated_at": datetime.utcnow().isoformat() + "Z",
            "workflow_id": workflow_id,
        },
        # The most important bits:
        "DESIGN_NAME": top_module,
        "VERILOG_FILES": verilog_sources,
        "CLOCK_PORT": clk_name,
        "CLOCK_PERIOD": clk_period_ns,
        "SYNTH_STRATEGY": "AREA 0",   # conservative default (can tune later)
        "PNR_SDC_FILE": "constraints/top.sdc",
    }

    _write_local(config_path, json.dumps(config, indent=2))

    # ---------- Docker run.sh (rerunnable contract) ----------
    # Host PDK root: your real path is backend/pdk (you already created it)
    # We keep it configurable.
    default_pdk_host = os.getenv("CHIPLOOP_PDK_ROOT_HOST") or os.path.abspath(os.path.join("backend", "pdk"))
    pdk_variant = state.get("pdk_variant") or DEFAULT_PDK_VARIANT
    openlane_image = state.get("openlane_image") or DEFAULT_OPENLANE_IMAGE

    # Runs folder inside stage_dir
    # OpenLane2 will create a "runs/<tag>" directory under CWD by default.
    run_tag = f"synth_{workflow_id}"

    run_sh_path = os.path.join(stage_dir, "run.sh")
    run_sh = f"""#!/usr/bin/env bash
set -euo pipefail

echo "== ChipLoop: {AGENT_NAME} =="
echo "PDK_VARIANT={pdk_variant}"
echo "OPENLANE_IMAGE={openlane_image}"
echo "WORKDIR=/work"
echo

docker run --rm \\
  -v "{default_pdk_host}:/pdk" \\
  -v "{os.path.abspath(stage_dir)}:/work" \\
  -e PDK_ROOT=/pdk \\
  -e PDK={pdk_variant} \\
  -e OPENLANE_NUM_CORES={DEFAULT_NUM_CORES} \\
  "{openlane_image}" \\
  bash -lc 'cd /work && openlane --pdk {pdk_variant} --flow Classic --to Yosys.Synthesis config.json'

echo
echo "Done. Inspect /work/runs/{run_tag} or latest run folder under /work/runs/"
"""
    _write_local(run_sh_path, run_sh)
    os.chmod(run_sh_path, 0o755)

    # ---------- Execute synthesis (best-effort) ----------
    # This is the first production step; we run it now and capture logs.
    exec_log_path = os.path.join(logs_dir, "openlane_synth.log")
    rc, out = _run(["bash", "-lc", f"{run_sh_path}"], cwd=stage_dir)
    _write_local(exec_log_path, out)

    # ---------- Collect best-effort outputs ----------
    # We don‚Äôt fake timing. We only parse what exists.
    runs_dir = os.path.join(stage_dir, "runs")
    metrics_json = None
    netlist_candidates = []

    if os.path.isdir(runs_dir):
        # find newest run folder
        run_folders = [os.path.join(runs_dir, d) for d in os.listdir(runs_dir) if os.path.isdir(os.path.join(runs_dir, d))]
        run_folders.sort(key=lambda p: os.path.getmtime(p))
        latest = run_folders[-1] if run_folders else None

        if latest:
            mj = os.path.join(latest, "final", "metrics.json")
            if os.path.exists(mj):
                metrics_json = mj

            # synthesis step folders often contain gate-level netlists
            netlist_candidates = glob.glob(os.path.join(latest, "*yosys-synthesis", "*.v")) + \
                                 glob.glob(os.path.join(latest, "*yosys-synthesis", "outputs", "*.v"))

    summary = {
        "workflow_id": workflow_id,
        "agent": AGENT_NAME,
        "status": "ok" if rc == 0 else "failed",
        "return_code": rc,
        "inputs": {
            "rtl_files": [os.path.basename(x) for x in copied],
            "top_module": top_module,
            "clock_port": clk_name,
            "clock_period_ns": clk_period_ns,
            "pdk_variant": pdk_variant,
            "openlane_image": openlane_image,
            "pdk_root_host": default_pdk_host,
        },
        "outputs": {
            "stage_dir": stage_dir,
            "config_json": config_path,
            "sdc": sdc_path,
            "run_sh": run_sh_path,
            "exec_log": exec_log_path,
            "metrics_json": metrics_json,
            "netlist_candidates": netlist_candidates[:10],
        }
    }

    summary_json_path = os.path.join(stage_dir, "synth_summary.json")
    summary_md_path = os.path.join(stage_dir, "synth_summary.md")
    _write_local(summary_json_path, json.dumps(summary, indent=2))

    md = f"""# Digital Synthesis Summary

- **Workflow**: {workflow_id}
- **Status**: `{summary["status"]}` (rc={rc})
- **Top module**: `{top_module}`
- **Clock**: `{clk_name}` @ **{clk_period_ns:.3f} ns**
- **PDK**: `{pdk_variant}`
- **Image**: `{openlane_image}`

## Deterministic outputs (rerunnable)
- `digital/synth/config.json`
- `digital/synth/constraints/top.sdc`
- `digital/synth/run.sh`
- `digital/synth/logs/openlane_synth.log`

## Parsed outputs (best-effort)
- metrics.json: `{metrics_json or "not found"}`
- netlist candidates: {len(netlist_candidates)} found
"""
    _write_local(summary_md_path, md)

    # ---------- Upload key artifacts to Supabase Storage ----------
    # (Option A: store heavy locally, upload key collaterals + summaries)
    try:
        save_text_artifact_and_record(workflow_id, AGENT_NAME, "digital", "synth/config.json", json.dumps(config, indent=2))
        save_text_artifact_and_record(workflow_id, AGENT_NAME, "digital", "synth/constraints/top.sdc", sdc)
        save_text_artifact_and_record(workflow_id, AGENT_NAME, "digital", "synth/run.sh", run_sh)
        save_text_artifact_and_record(workflow_id, AGENT_NAME, "digital", "synth/logs/openlane_synth.log", out)
        save_text_artifact_and_record(workflow_id, AGENT_NAME, "digital", "synth/synth_summary.json", json.dumps(summary, indent=2))
        save_text_artifact_and_record(workflow_id, AGENT_NAME, "digital", "synth/synth_summary.md", md)
        print("‚úÖ Uploaded synthesis artifacts to Supabase.")
    except Exception as e:
        print(f"‚ö†Ô∏è Synthesis artifact upload failed: {e}")

    # ---------- Update state for downstream workflow ----------
    digital = state.setdefault("digital", {})
    digital["synth"] = {
        "stage_dir": stage_dir,
        "summary_json": summary_json_path,
        "summary_md": summary_md_path,
        "metrics_json": metrics_json,
        "netlist_candidates": netlist_candidates[:10],
        "status": summary["status"],
    }

    state["status"] = f"{AGENT_NAME}: {summary['status']}"
    return state